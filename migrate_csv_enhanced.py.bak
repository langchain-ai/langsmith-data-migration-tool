#!/usr/bin/env python3
import argparse
import csv
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
from datetime import datetime
import pandas as pd
from langsmith import Client
from langsmith.schemas import DataType, Run, Example
from langsmith.evaluation import LangChainStringEvaluator, evaluate
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.prompt import Prompt, Confirm
from rich.table import Table
from rich.panel import Panel
from dotenv import load_dotenv
import time
import traceback
from collections import defaultdict
import uuid
import urllib3
import warnings

load_dotenv()

console = Console()


class EnhancedCSVMigrator:
    def __init__(self, api_key: Optional[str] = None, api_url: Optional[str] = None, project_id: Optional[str] = None, verify_ssl: bool = True):
        self.api_key = api_key or os.getenv("LANGSMITH_API_KEY")
        self.api_url = api_url or os.getenv("LANGSMITH_API_URL", "https://api.smith.langchain.com")
        # Ensure /api/v1 is appended if not present
        if not self.api_url.endswith('/api/v1'):
            self.api_url = self.api_url.rstrip('/') + '/api/v1'
        self.project_id = project_id or os.getenv("LANGSMITH_PROJECT_ID")
        self.verify_ssl = verify_ssl
        
        # Disable SSL warnings if verification is turned off
        if not self.verify_ssl:
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
            warnings.filterwarnings('ignore', message='Unverified HTTPS request')
            console.print("[yellow]Warning: SSL certificate verification is disabled[/yellow]")
        
        if not self.api_key:
            raise ValueError("LANGSMITH_API_KEY not provided or found in environment")
        
        # Initialize client with project if provided
        # Note: The api_url should already have /api/v1 appended from __init__
        # Remove /api/v1 for Client initialization as it adds it automatically
        client_url = self.api_url.replace('/api/v1', '')
        
        if self.project_id:
            os.environ["LANGCHAIN_PROJECT"] = self.project_id
            self.client = Client(api_key=self.api_key, api_url=client_url)
            console.print(f"[green]Connected to LangSmith at {client_url} - project: {self.project_id}[/green]")
        else:
            self.client = Client(api_key=self.api_key, api_url=client_url)
            console.print(f"[green]Connected to LangSmith at {client_url}[/green]")
        
        self.dataset_mapping = {}  # old_id -> new_id
        self.evaluator_mapping = {}  # rule_id -> evaluator_id
        self.evaluator_configs = {}  # dataset_id -> list of evaluator configurations
        self.experiment_mapping = {}  # old_experiment_id -> new_experiment_id
        self.annotation_queue_mapping = {}  # old_id -> new_id
        self.migration_report = {
            'datasets': {'created': [], 'failed': [], 'skipped': []},
            'examples': {'created': 0, 'failed': 0, 'by_dataset': {}},
            'run_rules': {'created': [], 'failed': [], 'with_evaluators': []},
            'evaluators': {'created': [], 'failed': []},
            'experiments': {'found': 0, 'migrated': 0, 'failed': 0},
            'annotation_queues': {'found': 0, 'mapped': 0},
            'alerts': {'found': 0, 'configured': 0},
            'validation_errors': []
        }
        
        self.stats = {
            'datasets_created': 0,
            'datasets_failed': 0,
            'examples_created': 0,
            'examples_failed': 0,
            'run_rules_created': 0,
            'run_rules_failed': 0,
            'evaluators_created': 0,
            'evaluators_failed': 0
        }
    
    def validate_csv_data(self, csv_dir: Path) -> Tuple[bool, List[str]]:
        """Validate CSV data integrity before migration"""
        console.print(Panel.fit("[bold]Validating CSV Data[/bold]"))
        errors = []
        
        # Check required files exist
        required_files = ['dataset.csv', 'examples_tagged.csv', 'run_rules.csv']
        for file in required_files:
            if not (csv_dir / file).exists():
                errors.append(f"Required file {file} not found")
        
        if errors:
            return False, errors
        
        # Load data for validation
        try:
            datasets_df = pd.read_csv(csv_dir / 'dataset.csv')
            examples_df = pd.read_csv(csv_dir / 'examples_tagged.csv')
            run_rules_df = pd.read_csv(csv_dir / 'run_rules.csv')
            
            # Check for required columns
            dataset_cols = {'id', 'name', 'data_type'}
            if not dataset_cols.issubset(set(datasets_df.columns)):
                errors.append(f"Dataset CSV missing required columns: {dataset_cols - set(datasets_df.columns)}")
            
            example_cols = {'id', 'dataset_id', 'inputs'}
            if not example_cols.issubset(set(examples_df.columns)):
                errors.append(f"Examples CSV missing required columns: {example_cols - set(examples_df.columns)}")
            
            # Check dataset references in examples
            dataset_ids = set(datasets_df['id'].unique())
            example_dataset_ids = set(examples_df['dataset_id'].unique())
            orphan_examples = example_dataset_ids - dataset_ids
            
            if orphan_examples:
                console.print(f"[yellow]Warning: {len(orphan_examples)} examples reference non-existent datasets[/yellow]")
                self.migration_report['validation_errors'].append(
                    f"Found {len(orphan_examples)} examples referencing missing datasets"
                )
            
            # Validate JSON fields
            json_fields_to_check = [
                (datasets_df, 'inputs_schema_definition'),
                (datasets_df, 'outputs_schema_definition'),
                (examples_df, 'inputs'),
                (examples_df, 'outputs'),
                (examples_df, 'metadata'),
                (run_rules_df, 'evaluators'),
                (run_rules_df, 'alerts')
            ]
            
            for df, field in json_fields_to_check:
                if field in df.columns:
                    invalid_json = 0
                    for idx, value in df[field].dropna().items():
                        if isinstance(value, str) and value.strip():
                            try:
                                json.loads(value)
                            except json.JSONDecodeError:
                                invalid_json += 1
                    if invalid_json > 0:
                        console.print(f"[yellow]Warning: {invalid_json} invalid JSON values in {field}[/yellow]")
            
            console.print(f"[green]✓[/green] Validation completed with {len(errors)} errors")
            
        except Exception as e:
            errors.append(f"Validation failed: {str(e)}")
        
        return len(errors) == 0, errors
    
    def migrate_all_datasets(self, csv_path: str) -> Dict[str, str]:
        """Migrate datasets and ensure all referenced datasets are created"""
        console.print(Panel.fit("[bold]Migrating All Datasets[/bold]"))
        
        # First, load both datasets and examples to find all referenced datasets
        datasets_df = pd.read_csv(csv_path)
        examples_csv = csv_path.replace('dataset.csv', 'examples_tagged.csv')
        
        all_dataset_ids = set(datasets_df['id'].unique())
        
        if os.path.exists(examples_csv):
            examples_df = pd.read_csv(examples_csv)
            referenced_dataset_ids = set(examples_df['dataset_id'].unique())
            
            # Find datasets referenced by examples but not in dataset.csv
            missing_datasets = referenced_dataset_ids - all_dataset_ids
            if missing_datasets:
                console.print(f"[yellow]Found {len(missing_datasets)} datasets referenced by examples but not in dataset.csv[/yellow]")
                console.print(f"[yellow]Creating placeholder datasets for missing references[/yellow]")
                
                # Create placeholder datasets for missing references
                for dataset_id in missing_datasets:
                    try:
                        dataset_name = f"migrated-dataset-{dataset_id[:8]}"
                        dataset = self.client.create_dataset(
                            dataset_name=dataset_name,
                            description=f"Auto-created dataset for migrated examples (original ID: {dataset_id})",
                            data_type=DataType.kv
                        )
                        self.dataset_mapping[dataset_id] = str(dataset.id)
                        self.stats['datasets_created'] += 1
                        self.migration_report['datasets']['created'].append({
                            'name': dataset_name,
                            'id': str(dataset.id),
                            'auto_created': True
                        })
                        console.print(f"[green]✓[/green] Created placeholder dataset: {dataset_name}")
                    except Exception as e:
                        console.print(f"[red]✗[/red] Failed to create placeholder dataset for {dataset_id}: {str(e)}")
                        self.stats['datasets_failed'] += 1
        
        console.print(f"Found {len(datasets_df)} datasets to migrate")
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=console
        ) as progress:
            task = progress.add_task("[cyan]Migrating datasets...", total=len(datasets_df))
            
            for _, row in datasets_df.iterrows():
                try:
                    dataset_name = row['name']
                    
                    # Check if already migrated
                    if row['id'] in self.dataset_mapping:
                        progress.update(task, advance=1)
                        continue
                    
                    # Check if dataset already exists
                    existing_datasets = list(self.client.list_datasets(dataset_name=dataset_name))
                    if existing_datasets:
                        console.print(f"[yellow]⚠[/yellow] Dataset '{dataset_name}' already exists, mapping to existing")
                        self.dataset_mapping[row['id']] = str(existing_datasets[0].id)
                        self.migration_report['datasets']['skipped'].append(dataset_name)
                        progress.update(task, advance=1)
                        continue
                    
                    # Parse schemas
                    inputs_schema = None
                    outputs_schema = None
                    
                    if pd.notna(row.get('inputs_schema_definition')):
                        try:
                            inputs_schema = json.loads(row['inputs_schema_definition'])
                        except:
                            pass
                    
                    if pd.notna(row.get('outputs_schema_definition')):
                        try:
                            outputs_schema = json.loads(row['outputs_schema_definition'])
                        except:
                            pass
                    
                    # Convert data_type
                    data_type_str = row.get('data_type', 'kv')
                    data_type = DataType.kv
                    if data_type_str == 'llm':
                        data_type = DataType.llm
                    elif data_type_str == 'chat':
                        data_type = DataType.chat
                    
                    # Create dataset
                    dataset = self.client.create_dataset(
                        dataset_name=dataset_name,
                        description=row.get('description', '') if pd.notna(row.get('description')) else '',
                        data_type=data_type,
                        inputs_schema=inputs_schema,
                        outputs_schema=outputs_schema
                    )
                    
                    self.dataset_mapping[row['id']] = str(dataset.id)
                    self.stats['datasets_created'] += 1
                    self.migration_report['datasets']['created'].append({
                        'name': dataset_name,
                        'id': str(dataset.id),
                        'auto_created': False
                    })
                    console.print(f"[green]✓[/green] Created dataset: {dataset_name}")
                    
                except Exception as e:
                    self.stats['datasets_failed'] += 1
                    self.migration_report['datasets']['failed'].append({
                        'name': row.get('name', 'unknown'),
                        'error': str(e)
                    })
                    console.print(f"[red]✗[/red] Failed to create dataset {row.get('name', 'unknown')}")
                    console.print(f"[red]   Error: {str(e)}[/red]")
                    if os.getenv('DEBUG'):
                        console.print(f"[red]   Traceback: {traceback.format_exc()}[/red]")
                
                progress.update(task, advance=1)
        
        return self.dataset_mapping
    
    def migrate_examples_enhanced(self, csv_path: str, batch_size: int = 50):
        """Migrate examples with better error handling and reporting"""
        console.print(Panel.fit("[bold]Migrating Examples[/bold]"))
        
        df = pd.read_csv(csv_path)
        console.print(f"Found {len(df)} examples to migrate")
        
        # Track experiments found in examples
        experiments_found = set()
        
        examples_by_dataset = defaultdict(list)
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=console
        ) as progress:
            task = progress.add_task("[cyan]Preparing examples...", total=len(df))
            
            for _, row in df.iterrows():
                try:
                    original_dataset_id = row['dataset_id']
                    new_dataset_id = self.dataset_mapping.get(original_dataset_id, original_dataset_id)
                    
                    # Parse inputs
                    inputs = json.loads(row['inputs']) if isinstance(row['inputs'], str) else row['inputs']
                    
                    # Parse outputs if present
                    outputs = None
                    if pd.notna(row.get('outputs')):
                        try:
                            outputs = json.loads(row['outputs']) if isinstance(row['outputs'], str) else row['outputs']
                        except:
                            outputs = None
                    
                    # Parse metadata if present and track experiments
                    metadata = {}
                    if pd.notna(row.get('metadata')):
                        try:
                            metadata = json.loads(row['metadata']) if isinstance(row['metadata'], str) else {}
                            # Track experiment IDs
                            if 'ls_experiment_id' in metadata:
                                experiments_found.add(metadata['ls_experiment_id'])
                        except:
                            metadata = {}
                    
                    example_data = {
                        'inputs': inputs,
                        'outputs': outputs,
                        'metadata': metadata,
                        'attachments': row.get('attachments', {}) if 'attachments' in row else {},
                        'source_run_id': row.get('source_run_id') if pd.notna(row.get('source_run_id')) else None,
                        'original_id': row.get('id')
                    }
                    
                    examples_by_dataset[new_dataset_id].append(example_data)
                    
                except Exception as e:
                    self.stats['examples_failed'] += 1
                    console.print(f"[red]✗[/red] Failed to prepare example: {str(e)[:100]}")
                
                progress.update(task, advance=1)
        
        # Upload examples by dataset
        console.print(f"Uploading examples to {len(examples_by_dataset)} datasets")
        
        for dataset_id, examples in examples_by_dataset.items():
            dataset_name = f"Dataset {str(dataset_id)[:8]}..."
            
            # Initialize dataset stats
            if dataset_id not in self.migration_report['examples']['by_dataset']:
                self.migration_report['examples']['by_dataset'][dataset_id] = {
                    'total': len(examples),
                    'success': 0,
                    'failed': 0
                }
            
            with Progress(
                SpinnerColumn(),
                TextColumn(f"[progress.description]{dataset_name}"),
                BarColumn(),
                TaskProgressColumn(),
                console=console
            ) as progress:
                task = progress.add_task("[cyan]Uploading examples...", total=len(examples))
                
                for i in range(0, len(examples), batch_size):
                    batch = examples[i:i + batch_size]
                    
                    for example_data in batch:
                        try:
                            self.client.create_example(
                                inputs=example_data['inputs'],
                                outputs=example_data.get('outputs'),
                                dataset_id=str(dataset_id) if not isinstance(dataset_id, str) else dataset_id,
                                metadata=example_data.get('metadata', {}),
                                attachments=example_data.get('attachments', {}),
                                source_run_id=example_data.get('source_run_id')
                            )
                            self.stats['examples_created'] += 1
                            self.migration_report['examples']['created'] += 1
                            self.migration_report['examples']['by_dataset'][dataset_id]['success'] += 1
                        except Exception as e:
                            self.stats['examples_failed'] += 1
                            self.migration_report['examples']['failed'] += 1
                            self.migration_report['examples']['by_dataset'][dataset_id]['failed'] += 1
                            if os.getenv('DEBUG'):
                                console.print(f"[red]   Failed example: {str(e)[:100]}[/red]")
                    
                    progress.update(task, advance=len(batch))
        
        # Report on experiments found
        if experiments_found:
            self.migration_report['experiments']['found'] = len(experiments_found)
            console.print(f"[cyan]Found {len(experiments_found)} unique experiment IDs in examples metadata[/cyan]")
            for exp_id in list(experiments_found)[:5]:
                self.experiment_mapping[exp_id] = exp_id  # Map to same ID for tracking
            if len(experiments_found) > 5:
                console.print(f"[cyan]  (showing first 5 of {len(experiments_found)})[/cyan]")
        
        console.print(f"[green]✓[/green] Processed {self.stats['examples_created']} examples successfully")
        if self.stats['examples_failed'] > 0:
            console.print(f"[red]✗[/red] Failed to process {self.stats['examples_failed']} examples")
    
    def create_run_rule_with_evaluators(self, rule_row: pd.Series) -> bool:
        """Create a run rule in LangSmith with evaluators attached"""
        try:
            rule_id = rule_row['id']
            display_name = rule_row['display_name']
            
            # Map dataset IDs
            original_dataset_id = rule_row.get('dataset_id')
            dataset_id = self.dataset_mapping.get(original_dataset_id, original_dataset_id) if pd.notna(original_dataset_id) else None
            
            add_to_dataset_id = rule_row.get('add_to_dataset_id')
            new_add_to_dataset_id = self.dataset_mapping.get(add_to_dataset_id, add_to_dataset_id) if pd.notna(add_to_dataset_id) else None
            
            # Get session_id if present
            session_id = rule_row.get('session_id') if pd.notna(rule_row.get('session_id')) else None
            
            # Skip if no session_id or dataset_id
            if not session_id and not dataset_id:
                console.print(f"[yellow]Skipping rule '{display_name}': no session_id or dataset_id[/yellow]")
                return False
            
            # If we have a session_id but no dataset_id, skip for now
            # (session IDs from another environment won't work)
            if session_id and not dataset_id:
                console.print(f"[yellow]Skipping rule '{display_name}': session_id without dataset_id (session may not exist)[/yellow]")
                return False
            
            # Parse evaluators
            evaluators = []
            if pd.notna(rule_row.get('evaluators')):
                try:
                    evaluators_data = json.loads(rule_row['evaluators'])
                    if evaluators_data and isinstance(evaluators_data, list):
                        evaluators = evaluators_data
                except:
                    pass
            
            # Parse code evaluators
            code_evaluators = []
            if pd.notna(rule_row.get('code_evaluators')):
                try:
                    code_evaluators = json.loads(rule_row['code_evaluators'])
                except:
                    pass
            
            # Parse alerts
            alerts = []
            if pd.notna(rule_row.get('alerts')):
                try:
                    alerts = json.loads(rule_row['alerts'])
                except:
                    pass
            
            # Process evaluators to add OpenAI API key if needed
            if evaluators:
                for evaluator in evaluators:
                    if 'structured' in evaluator and 'model' in evaluator['structured']:
                        model_config = evaluator['structured']['model']
                        if 'kwargs' in model_config:
                            # Ensure OpenAI API key is set
                            if 'openai_api_key' not in model_config['kwargs'] or model_config['kwargs']['openai_api_key'] == {'id': ['OPENAI_API_KEY'], 'lc': 1, 'type': 'secret'}:
                                # Use environment variable or placeholder
                                openai_key = os.getenv('OPENAI_API_KEY', 'sk-dummy-key-for-migration')
                                model_config['kwargs']['openai_api_key'] = openai_key
            
            # Build the run rule request
            rule_data = {
                'display_name': display_name,
                'is_enabled': bool(rule_row.get('is_enabled', True)),
                'sampling_rate': float(rule_row.get('sampling_rate', 1.0)),
                'filter': rule_row.get('filter') if pd.notna(rule_row.get('filter')) else None,
                'trace_filter': rule_row.get('trace_filter') if pd.notna(rule_row.get('trace_filter')) else None,
                'tree_filter': rule_row.get('tree_filter') if pd.notna(rule_row.get('tree_filter')) else None,
                'add_to_annotation_queue_id': rule_row.get('add_to_annotation_queue_id') if pd.notna(rule_row.get('add_to_annotation_queue_id')) else None,
                'add_to_dataset_id': new_add_to_dataset_id,
                'evaluators': evaluators,
                'code_evaluators': code_evaluators,
                'alerts': alerts
            }
            
            # Add session_id or dataset_id (at least one is required)
            if session_id:
                rule_data['session_id'] = session_id
            if dataset_id:
                rule_data['dataset_id'] = dataset_id
            
            # Remove None values
            rule_data = {k: v for k, v in rule_data.items() if v is not None}
            
            # Create the run rule via API
            import requests
            headers = {
                'x-api-key': self.api_key,
                'Content-Type': 'application/json'
            }
            
            # The api_url already has /api/v1 appended, so just use the endpoint
            response = requests.post(
                f"{self.api_url}/runs/rules",
                headers=headers,
                json=rule_data,
                verify=self.verify_ssl
            )
            
            if response.status_code == 200 or response.status_code == 201:
                console.print(f"[green]✓ Created run rule '{display_name}' with {len(evaluators)} evaluators[/green]")
                if dataset_id:
                    console.print(f"  [cyan]Attached to dataset: {dataset_id}[/cyan]")
                return True
            elif response.status_code == 404 and 'session' in response.text.lower():
                # Session not found - this is expected when migrating between environments
                console.print(f"[yellow]⚠ Rule '{display_name}': session {session_id[:8] if session_id else 'unknown'}... not found (expected when migrating)[/yellow]")
                return None  # Return None to indicate skipped due to missing session
            else:
                console.print(f"[red]Failed to create run rule '{display_name}': {response.status_code}[/red]")
                if response.text:
                    error_text = response.text[:200]
                    if 'openai' in error_text.lower():
                        console.print(f"  [red]OpenAI API key issue - set OPENAI_API_KEY environment variable[/red]")
                    else:
                        console.print(f"  [red]{error_text}[/red]")
                return False
                
        except Exception as e:
            console.print(f"[red]Failed to create run rule: {str(e)}[/red]")
            return False
    
    def create_and_attach_evaluator(self, evaluator_config: Dict[str, Any], rule_name: str, dataset_id: str) -> Optional[str]:
        """Create an evaluator configuration for a dataset and prepare for test creation"""
        try:
            if not dataset_id:
                console.print(f"[yellow]No dataset ID for evaluator {rule_name}[/yellow]")
                return None
            
            # Verify dataset exists in our mapping
            if dataset_id not in self.dataset_mapping.values():
                console.print(f"[yellow]Dataset {dataset_id} not in mapping, skipping evaluator[/yellow]")
                return None
            
            evaluator_name = f"{rule_name}_evaluator"
            
            if 'structured' in evaluator_config:
                # Handle structured evaluator
                structured = evaluator_config['structured']
                
                # Extract evaluator configuration
                model_config = structured.get('model', {})
                prompt = structured.get('prompt', [])
                schema = structured.get('schema', {})
                variable_mapping = structured.get('variable_mapping', {})
                
                # Create the evaluation configuration tied to the dataset
                eval_config = {
                    'evaluator_name': evaluator_name,
                    'dataset_id': dataset_id,
                    'rule_name': rule_name,
                    'type': 'structured',
                    'model_config': model_config,
                    'prompt': prompt,
                    'schema': schema,
                    'variable_mapping': variable_mapping,
                    'created_at': datetime.now().isoformat()
                }
                
                # Store configuration for the dataset
                if dataset_id not in self.evaluator_configs:
                    self.evaluator_configs[dataset_id] = []
                self.evaluator_configs[dataset_id].append(eval_config)
                
                # Try to get dataset name for better logging
                try:
                    dataset = self.client.read_dataset(dataset_id=dataset_id)
                    dataset_name = dataset.name
                except:
                    dataset_name = f"Dataset_{dataset_id[:8]}"
                
                console.print(f"[green]✓ Configured evaluator '{evaluator_name}' for dataset '{dataset_name}'[/green]")
                
                # Create a simple evaluator function that could be used with run_on_dataset
                # This is stored for reference but actual implementation would need the model
                self.migration_report['evaluators']['created'].append({
                    'name': evaluator_name,
                    'dataset': dataset_name,
                    'dataset_id': dataset_id,
                    'type': 'structured',
                    'config_saved': True
                })
                
                return evaluator_name
            
            elif 'code' in evaluator_config:
                # Handle code evaluator
                code = evaluator_config['code']
                
                eval_config = {
                    'evaluator_name': evaluator_name,
                    'dataset_id': dataset_id,
                    'rule_name': rule_name,
                    'type': 'code',
                    'code': code,
                    'created_at': datetime.now().isoformat()
                }
                
                if dataset_id not in self.evaluator_configs:
                    self.evaluator_configs[dataset_id] = []
                self.evaluator_configs[dataset_id].append(eval_config)
                
                # Try to get dataset name
                try:
                    dataset = self.client.read_dataset(dataset_id=dataset_id)
                    dataset_name = dataset.name
                except:
                    dataset_name = f"Dataset_{dataset_id[:8]}"
                
                console.print(f"[green]✓ Configured code evaluator '{evaluator_name}' for dataset '{dataset_name}'[/green]")
                
                self.migration_report['evaluators']['created'].append({
                    'name': evaluator_name,
                    'dataset': dataset_name,
                    'dataset_id': dataset_id,
                    'type': 'code',
                    'config_saved': True
                })
                
                return evaluator_name
                
        except Exception as e:
            console.print(f"[red]Failed to create evaluator: {str(e)}[/red]")
            return None
    
    def migrate_annotation_queues_and_alerts(self, csv_path: str):
        """Process annotation queues and alerts from run rules"""
        console.print(Panel.fit("[bold]Processing Annotation Queues and Alerts[/bold]"))
        
        df = pd.read_csv(csv_path)
        
        # Process annotation queues
        queue_ids = df['add_to_annotation_queue_id'].dropna().unique()
        if len(queue_ids) > 0:
            console.print(f"[cyan]Found {len(queue_ids)} unique annotation queue references[/cyan]")
            self.migration_report['annotation_queues']['found'] = len(queue_ids)
            
            for queue_id in queue_ids:
                # Map annotation queue IDs (in production, would create actual queues)
                self.annotation_queue_mapping[queue_id] = queue_id
                self.migration_report['annotation_queues']['mapped'] += 1
                
                rules_using_queue = df[df['add_to_annotation_queue_id'] == queue_id]
                console.print(f"[green]Queue {queue_id[:8]}... referenced by {len(rules_using_queue)} rules[/green]")
        
        # Process alerts
        alerts_count = 0
        for _, rule in df.iterrows():
            if pd.notna(rule.get('alerts')):
                try:
                    alerts = json.loads(rule['alerts'])
                    if alerts:
                        alerts_count += 1
                        self.migration_report['alerts']['found'] += 1
                        self.migration_report['alerts']['configured'] += 1
                        console.print(f"[green]Found alert configuration for rule: {rule['display_name']}[/green]")
                except:
                    pass
        
        if alerts_count > 0:
            console.print(f"[cyan]Processed {alerts_count} rules with alert configurations[/cyan]")
    
    def migrate_run_rules_with_evaluators(self, csv_path: str, create_rules: bool = False):
        """Migrate run rules and create associated evaluators
        
        Args:
            csv_path: Path to run_rules.csv
            create_rules: If True, actually create run rules in LangSmith via API
        """
        console.print(Panel.fit("[bold]Migrating Run Rules with Evaluators[/bold]"))
        
        # First process annotation queues and alerts
        self.migrate_annotation_queues_and_alerts(csv_path)
        
        df = pd.read_csv(csv_path)
        console.print(f"Found {len(df)} run rules")
        
        if create_rules:
            console.print("[cyan]Creating run rules in LangSmith...[/cyan]")
        
        rules_with_evaluators = []
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=console
        ) as progress:
            task = progress.add_task("[cyan]Processing run rules...", total=len(df))
            
            for _, row in df.iterrows():
                try:
                    rule_id = row['id']
                    display_name = row['display_name']
                    
                    # Map dataset IDs
                    original_dataset_id = row.get('dataset_id')
                    new_dataset_id = self.dataset_mapping.get(original_dataset_id, original_dataset_id) if pd.notna(original_dataset_id) else None
                    
                    add_to_dataset_id = row.get('add_to_dataset_id')
                    new_add_to_dataset_id = self.dataset_mapping.get(add_to_dataset_id, add_to_dataset_id) if pd.notna(add_to_dataset_id) else None
                    
                    # Parse evaluators
                    evaluators = []
                    evaluator_ids = []
                    
                    if pd.notna(row.get('evaluators')):
                        try:
                            evaluators_data = json.loads(row['evaluators'])
                            if evaluators_data and isinstance(evaluators_data, list):
                                for eval_config in evaluators_data:
                                    # Use the dataset_id for evaluator creation
                                    eval_dataset_id = new_dataset_id or new_add_to_dataset_id
                                    if eval_dataset_id:
                                        evaluator_name = self.create_and_attach_evaluator(
                                            eval_config, 
                                            display_name,
                                            eval_dataset_id
                                        )
                                        if evaluator_name:
                                            evaluator_ids.append(evaluator_name)
                                            self.stats['evaluators_created'] += 1
                        except json.JSONDecodeError:
                            pass
                    
                    # Parse code evaluators
                    if pd.notna(row.get('code_evaluators')):
                        try:
                            code_evaluators = json.loads(row['code_evaluators'])
                            if code_evaluators:
                                evaluator_id = f"{display_name}_code_evaluator"
                                evaluator_ids.append(evaluator_id)
                                console.print(f"[blue]Would create code evaluator: {evaluator_id}[/blue]")
                        except:
                            pass
                    
                    rule_data = {
                        'id': rule_id,
                        'display_name': display_name,
                        'filter': row.get('filter'),
                        'sampling_rate': row.get('sampling_rate', 1.0),
                        'session_id': row.get('session_id'),
                        'dataset_id': new_dataset_id,
                        'add_to_dataset_id': new_add_to_dataset_id,
                        'evaluator_ids': evaluator_ids,
                        'has_evaluators': len(evaluator_ids) > 0
                    }
                    
                    rules_with_evaluators.append(rule_data)
                    
                    if len(evaluator_ids) > 0:
                        self.migration_report['run_rules']['with_evaluators'].append(display_name)
                        self.evaluator_mapping[rule_id] = evaluator_ids
                    
                    self.stats['run_rules_created'] += 1
                    self.migration_report['run_rules']['created'].append(display_name)
                    
                    console.print(f"[green]✓[/green] Processed run rule: {display_name}" + 
                                (f" (with {len(evaluator_ids)} evaluators)" if evaluator_ids else ""))
                    
                except Exception as e:
                    self.stats['run_rules_failed'] += 1
                    self.migration_report['run_rules']['failed'].append({
                        'name': row.get('display_name', 'unknown'),
                        'error': str(e)
                    })
                    console.print(f"[red]✗[/red] Failed to process run rule: {str(e)[:100]}")
                
                progress.update(task, advance=1)
        
        # Actually create run rules if requested
        if create_rules:
            console.print("\n[bold]Creating Run Rules in LangSmith[/bold]")
            created_count = 0
            skipped_count = 0
            failed_count = 0
            session_not_found = 0
            
            for _, row in df.iterrows():
                result = self.create_run_rule_with_evaluators(row)
                if result is True:
                    created_count += 1
                elif result is False:
                    failed_count += 1
                elif result is None:
                    session_not_found += 1
                else:
                    skipped_count += 1
            
            console.print(f"\n[bold]Run Rule Creation Summary:[/bold]")
            console.print(f"  [green]✓ Created: {created_count}[/green]")
            if session_not_found > 0:
                console.print(f"  [yellow]⚠ Session not found: {session_not_found} (expected when migrating)[/yellow]")
            if skipped_count > 0:
                console.print(f"  [yellow]⊘ Skipped: {skipped_count}[/yellow]")
            if failed_count > 0:
                console.print(f"  [red]✗ Failed: {failed_count}[/red]")
            
            if os.getenv('OPENAI_API_KEY') is None:
                console.print("\n[yellow]Note: Set OPENAI_API_KEY environment variable for evaluators to work properly[/yellow]")
        
        # Save enhanced run rules data
        output_file = "enhanced_run_rules.json"
        with open(output_file, 'w') as f:
            json.dump(rules_with_evaluators, f, indent=2, default=str)
        
        console.print(f"[green]✓[/green] Processed run rules saved to {output_file}")
        console.print(f"[yellow]Note: {len([r for r in rules_with_evaluators if r['has_evaluators']])} run rules have evaluators configured[/yellow]")
        
        # Save evaluator configurations grouped by dataset
        if self.evaluator_configs:
            evaluator_file = "dataset_evaluator_configs.json"
            
            # Create summary of evaluators per dataset
            evaluator_summary = {}
            for dataset_id, configs in self.evaluator_configs.items():
                dataset_name = "Unknown"
                # Try to find dataset name
                for old_id, new_id in self.dataset_mapping.items():
                    if new_id == dataset_id:
                        dataset_name = f"Dataset_{old_id[:8]}"
                        break
                
                evaluator_summary[dataset_id] = {
                    'dataset_name': dataset_name,
                    'dataset_id': dataset_id,
                    'evaluator_count': len(configs),
                    'evaluators': configs
                }
            
            with open(evaluator_file, 'w') as f:
                json.dump(evaluator_summary, f, indent=2, default=str)
            
            total_evaluators = sum(len(configs) for configs in self.evaluator_configs.values())
            console.print(f"[green]✓[/green] {total_evaluators} evaluators attached to {len(self.evaluator_configs)} datasets")
            console.print(f"[green]✓[/green] Dataset-evaluator mappings saved to {evaluator_file}")
    
    def create_dataset_tests_with_evaluators(self):
        """Create test configurations and example run script for datasets with evaluators"""
        if not self.evaluator_configs:
            return
        
        console.print(Panel.fit("[bold]Creating Dataset Test Configurations and Run Script[/bold]"))
        
        test_configs = []
        
        for dataset_id, evaluators in self.evaluator_configs.items():
            try:
                # Verify dataset exists
                dataset = self.client.read_dataset(dataset_id=dataset_id)
                
                # Create test configuration
                test_config = {
                    'dataset_id': dataset_id,
                    'dataset_name': dataset.name,
                    'evaluators': evaluators,
                    'test_name': f"Test_{dataset.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    'created_at': datetime.now().isoformat()
                }
                
                test_configs.append(test_config)
                
                console.print(f"[green]✓ Created test config for dataset '{dataset.name}' with {len(evaluators)} evaluators[/green]")
                
            except Exception as e:
                console.print(f"[red]Failed to create test config for dataset {dataset_id}: {str(e)}[/red]")
        
        if test_configs:
            # Save test configurations
            test_file = "dataset_test_configs.json"
            with open(test_file, 'w') as f:
                json.dump(test_configs, f, indent=2, default=str)
            
            console.print(f"[green]✓ Test configurations saved to {test_file}[/green]")
            
            # Create a Python script to run the tests
            script_content = '''#!/usr/bin/env python3
"""Run tests on migrated datasets with their configured evaluators"""

import json
from langsmith import Client
from langsmith.evaluation import run_on_dataset

# Load test configurations
with open("dataset_test_configs.json", "r") as f:
    test_configs = json.load(f)

client = Client()

def dummy_model(inputs):
    """Replace with your actual model/chain"""
    return {"output": "This is a placeholder response"}

for config in test_configs:
    dataset_id = config["dataset_id"]
    dataset_name = config["dataset_name"]
    
    print(f"Running test on dataset: {dataset_name}")
    
    # Create evaluators from config
    evaluators = []
    for eval_config in config["evaluators"]:
        # Create evaluator based on type
        # This is where you would implement actual evaluator creation
        pass
    
    # Run evaluation
    # results = run_on_dataset(
    #     client=client,
    #     dataset_name=dataset_name,
    #     llm_or_chain_factory=dummy_model,
    #     evaluation=evaluators,
    #     project_name=config["test_name"]
    # )
    
    print(f"Test configuration ready for dataset: {dataset_name}")
    print(f"  - Dataset ID: {dataset_id}")
    print(f"  - Evaluators: {len(config['evaluators'])}")
'''
            
            script_file = "run_dataset_tests.py"
            with open(script_file, 'w') as f:
                f.write(script_content)
            
            console.print(f"[green]✓ Test run script saved to {script_file}[/green]")
            console.print(f"[cyan]Created {len(test_configs)} test configurations[/cyan]")
            console.print(f"[yellow]Note: Edit {script_file} to add your model/chain for testing[/yellow]")
    
    def migrate_experiment_results(self, csv_path: str):
        """Migrate existing experiment results from examples with experiment IDs"""
        console.print(Panel.fit("[bold]Migrating Experiment Results[/bold]"))
        
        df = pd.read_csv(csv_path)
        experiments_data = defaultdict(list)
        
        # Group examples by experiment ID
        for _, row in df.iterrows():
            if pd.notna(row.get('metadata')):
                try:
                    metadata = json.loads(row['metadata'])
                    if 'ls_experiment_id' in metadata:
                        exp_id = metadata['ls_experiment_id']
                        experiments_data[exp_id].append({
                            'example_id': row['id'],
                            'dataset_id': row['dataset_id'],
                            'inputs': json.loads(row['inputs']) if isinstance(row['inputs'], str) else row['inputs'],
                            'outputs': json.loads(row['outputs']) if pd.notna(row.get('outputs')) and isinstance(row['outputs'], str) else row.get('outputs'),
                            'metadata': metadata
                        })
                except:
                    pass
        
        if not experiments_data:
            console.print("[yellow]No experiment data found in examples[/yellow]")
            return
        
        for exp_id, examples in experiments_data.items():
            console.print(f"[cyan]Processing experiment {exp_id} with {len(examples)} examples[/cyan]")
            
            # Get the dataset for this experiment
            dataset_ids = set(ex['dataset_id'] for ex in examples)
            dataset_id = list(dataset_ids)[0] if dataset_ids else None
            
            if dataset_id and dataset_id in self.dataset_mapping:
                new_dataset_id = self.dataset_mapping[dataset_id]
                
                # Create runs for each example in the experiment
                for example in examples:
                    try:
                        # Create a run with the experiment metadata
                        run_metadata = example['metadata'].copy()
                        run_id = run_metadata.get('run_id', str(uuid.uuid4()))
                        
                        # Note: In production, you would create actual runs here
                        # self.client.create_run(
                        #     name="experiment_run",
                        #     inputs=example['inputs'],
                        #     outputs=example['outputs'],
                        #     run_type="chain",
                        #     project_name=self.project_id,
                        #     reference_example_id=example['example_id'],
                        #     extra=run_metadata
                        # )
                        
                        console.print(f"[blue]  Would migrate run {run_id[:8]}... for experiment[/blue]")
                        
                    except Exception as e:
                        console.print(f"[red]  Failed to migrate run: {str(e)}[/red]")
                
                self.migration_report['experiments']['migrated'] += 1
            else:
                console.print(f"[yellow]  Dataset {dataset_id} not found in mapping[/yellow]")
    
    def create_annotation_queues_config(self):
        """Create configuration file for annotation queues to be created manually"""
        if not self.annotation_queue_mapping:
            return
        
        console.print(Panel.fit("[bold]Creating Annotation Queue Configuration[/bold]"))
        
        queue_config = {
            'annotation_queues': [],
            'instructions': 'Create these annotation queues in LangSmith UI',
            'timestamp': datetime.now().isoformat()
        }
        
        for queue_id in self.annotation_queue_mapping.keys():
            queue_config['annotation_queues'].append({
                'id': queue_id,
                'name': f"Queue_{queue_id[:8]}",
                'description': f"Migrated annotation queue (original ID: {queue_id})"
            })
        
        config_file = "annotation_queues_config.json"
        with open(config_file, 'w') as f:
            json.dump(queue_config, f, indent=2)
        
        console.print(f"[green]✓ Annotation queue configuration saved to {config_file}[/green]")
        console.print(f"[yellow]  Create {len(self.annotation_queue_mapping)} queues in LangSmith UI using this config[/yellow]")
    
    def display_migration_report(self):
        """Display comprehensive migration report"""
        console.print("\n" + "="*60)
        console.print(Panel.fit("[bold cyan]Migration Report[/bold cyan]"))
        
        # Summary table
        table = Table(title="Migration Summary", show_header=True, header_style="bold magenta")
        table.add_column("Item", style="cyan", no_wrap=True)
        table.add_column("Success", style="green")
        table.add_column("Failed", style="red")
        table.add_column("Notes", style="yellow")
        
        table.add_row(
            "Datasets", 
            str(self.stats['datasets_created']), 
            str(self.stats['datasets_failed']),
            f"{len(self.migration_report['datasets']['skipped'])} skipped (existing)"
        )
        table.add_row(
            "Examples", 
            str(self.stats['examples_created']), 
            str(self.stats['examples_failed']),
            f"Across {len(self.migration_report['examples']['by_dataset'])} datasets"
        )
        table.add_row(
            "Run Rules", 
            str(self.stats['run_rules_created']), 
            str(self.stats['run_rules_failed']),
            f"{len(self.migration_report['run_rules']['with_evaluators'])} with evaluators"
        )
        table.add_row(
            "Evaluators", 
            str(self.stats['evaluators_created']), 
            str(self.stats['evaluators_failed']),
            f"{len(self.evaluator_configs)} configured"
        )
        
        if self.migration_report['experiments']['found'] > 0:
            table.add_row(
                "Experiments",
                str(self.migration_report['experiments']['found']),
                "0",
                "Found in metadata"
            )
        
        if self.migration_report['annotation_queues']['found'] > 0:
            table.add_row(
                "Annotation Queues",
                str(self.migration_report['annotation_queues']['mapped']),
                "0",
                f"{self.migration_report['annotation_queues']['found']} found"
            )
        
        if self.migration_report['alerts']['found'] > 0:
            table.add_row(
                "Alert Configs",
                str(self.migration_report['alerts']['configured']),
                "0",
                f"{self.migration_report['alerts']['found']} found"
            )
        
        console.print(table)
        
        # Validation errors
        if self.migration_report['validation_errors']:
            console.print("\n[yellow]Validation Warnings:[/yellow]")
            for error in self.migration_report['validation_errors']:
                console.print(f"  • {error}")
        
        # Failed items details
        if self.migration_report['datasets']['failed']:
            console.print("\n[red]Failed Datasets:[/red]")
            for item in self.migration_report['datasets']['failed'][:5]:
                console.print(f"  • {item['name']}: {item['error'][:50]}")
            if len(self.migration_report['datasets']['failed']) > 5:
                console.print(f"  ... and {len(self.migration_report['datasets']['failed']) - 5} more")
        
        # Save full report to file
        report_file = f"migration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(self.migration_report, f, indent=2, default=str)
        
        console.print(f"\n[green]Full report saved to: {report_file}[/green]")
        
        # Save all mappings
        mappings_file = f"migration_mappings_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(mappings_file, 'w') as f:
            json.dump({
                'dataset_mapping': self.dataset_mapping,
                'evaluator_mapping': self.evaluator_mapping,
                'experiment_mapping': self.experiment_mapping,
                'annotation_queue_mapping': self.annotation_queue_mapping,
                'timestamp': datetime.now().isoformat()
            }, f, indent=2, default=str)
        
        console.print(f"[green]Mappings saved to: {mappings_file}[/green]")
        
        # Provide recommendations
        console.print("\n[bold yellow]Post-Migration Steps:[/bold yellow]")
        if self.migration_report['experiments']['found'] > 0:
            console.print(f"  • {self.migration_report['experiments']['found']} experiment IDs were found - create experiments using evaluator configs")
        if self.migration_report['annotation_queues']['found'] > 0:
            console.print(f"  • {self.migration_report['annotation_queues']['found']} annotation queues need to be created in LangSmith UI")
        if self.migration_report['alerts']['found'] > 0:
            console.print(f"  • {self.migration_report['alerts']['found']} alert configurations need to be set up in LangSmith UI")
        if self.evaluator_configs:
            total_evaluators = sum(len(configs) for configs in self.evaluator_configs.values())
            console.print(f"  • {total_evaluators} evaluators attached to {len(self.evaluator_configs)} datasets")
            console.print(f"  • Run experiments on these datasets to use the configured evaluators")


def main():
    parser = argparse.ArgumentParser(description='Enhanced CSV to LangSmith Migration Tool')
    parser.add_argument('--csv-dir', type=str, default='example_csv', help='Directory containing CSV files')
    parser.add_argument('--api-key', type=str, help='LangSmith API key')
    parser.add_argument('--api-url', type=str, default='https://api.smith.langchain.com', help='LangSmith API URL')
    parser.add_argument('--no-verify-ssl', action='store_true', help='Disable SSL certificate verification (use for self-hosted with self-signed certs)')
    parser.add_argument('--project-id', type=str, help='Destination LangSmith project ID')
    parser.add_argument('--validate-only', action='store_true', help='Only validate CSV data without migrating')
    parser.add_argument('--batch-size', type=int, default=50, help='Batch size for example uploads')
    parser.add_argument('--create-run-rules', action='store_true', help='Actually create run rules in LangSmith (with evaluators)')
    parser.add_argument('--debug', action='store_true', help='Enable debug output')
    
    args = parser.parse_args()
    
    if args.debug:
        os.environ['DEBUG'] = '1'
    
    console.print(Panel.fit("[bold cyan]Enhanced LangSmith CSV Migration Tool[/bold cyan]"))
    
    csv_dir = Path(args.csv_dir)
    if not csv_dir.exists():
        console.print(f"[red]CSV directory '{csv_dir}' not found[/red]")
        sys.exit(1)
    
    if not args.api_key:
        # Try to get from environment first
        args.api_key = os.getenv("LANGSMITH_API_KEY")
        if not args.api_key and sys.stdin.isatty():
            # Only prompt if we have a terminal
            try:
                args.api_key = Prompt.ask("\nEnter your LangSmith API key", password=True)
            except (EOFError, KeyboardInterrupt):
                pass
    
    if not args.api_key:
        console.print("[red]API key is required (set LANGSMITH_API_KEY environment variable or use --api-key)[/red]")
        sys.exit(1)
    
    migrator = EnhancedCSVMigrator(
        api_key=args.api_key, 
        api_url=args.api_url, 
        project_id=args.project_id,
        verify_ssl=not args.no_verify_ssl
    )
    
    # Validate data
    is_valid, errors = migrator.validate_csv_data(csv_dir)
    
    if not is_valid:
        console.print("[red]Validation failed with errors:[/red]")
        for error in errors:
            console.print(f"  • {error}")
        if not Confirm.ask("Continue with migration despite errors?"):
            sys.exit(1)
    
    if args.validate_only:
        console.print("[green]Validation completed[/green]")
        sys.exit(0)
    
    start_time = time.time()
    
    # Migrate datasets (including auto-creating missing ones)
    if (csv_dir / "dataset.csv").exists():
        migrator.migrate_all_datasets(str(csv_dir / "dataset.csv"))
    
    # Migrate examples
    if (csv_dir / "examples_tagged.csv").exists():
        migrator.migrate_examples_enhanced(str(csv_dir / "examples_tagged.csv"), batch_size=args.batch_size)
        # Migrate experiment results from examples
        migrator.migrate_experiment_results(str(csv_dir / "examples_tagged.csv"))
    
    # Migrate run rules with evaluators
    if (csv_dir / "run_rules.csv").exists():
        migrator.migrate_run_rules_with_evaluators(
            str(csv_dir / "run_rules.csv"),
            create_rules=args.create_run_rules
        )
    
    # Create configuration files for manual setup
    migrator.create_annotation_queues_config()
    
    # Create test configurations for datasets with evaluators
    migrator.create_dataset_tests_with_evaluators()
    
    elapsed_time = time.time() - start_time
    
    console.print(f"\n[bold]Migration completed in {elapsed_time:.2f} seconds[/bold]")
    migrator.display_migration_report()


if __name__ == "__main__":
    main()